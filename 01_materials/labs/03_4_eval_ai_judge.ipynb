{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fbe851e5",
   "metadata": {},
   "source": [
    "# AI as Judge\n",
    "\n",
    "[G-Eval](https://deepeval.com/docs/metrics-llm-evals) is a framework that uses LLM as a judge to evaluate LLM outputs. The evaluation can be based on any criteria. G-Eval is implemented by a library called [DeepEval](https://deepeval.com/) which includes a broader set of tests.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f0650f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%dotenv ../../05_src/.secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7bd5ec5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "import os\n",
    "\n",
    "document_folder = \"../../05_src/documents/\"\n",
    "blue_cross_file = \"the_blue_cross.txt\"\n",
    "file_path = os.path.join(document_folder, blue_cross_file)\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    blue_cross_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "843da553",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = \"You are an helpful assistant that summarizes works of fiction with a quirky and bubbly approach.\"\n",
    "PROMPT = \"\"\"\n",
    "    Summarize the following story in at most four paragraphs. Please include all key characters and plot points.\n",
    "    <story>\n",
    "    {story}\n",
    "    </story>\n",
    "    In addition to the summary, add an introduction paragraph where you greet the reader and a conclusion where you share an opinion about the story.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f951529",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4o-mini\",\n",
    "    instructions=instructions,\n",
    "    input=[\n",
    "        {\"role\": \"user\", \n",
    "         \"content\": PROMPT.format(story=blue_cross_text)}\n",
    "    ],\n",
    "    temperature=1.2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "269743e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello there, adventurous reader! ðŸŒŸ Buckle up for a whimsical ride through the clever corners of 'The Blue Cross,' where a detective board collides with a master criminal, and secrets flutter on the wind like fallen leavesâ€”let's dive in!\\n\\nIn this delightful tale, we meet Aristide Valentin, the shrewd head of the Paris police, whoâ€™s on an urgent mission to capture the notorious criminal Flambeau at the bustling Eucharistic Congress in London. Armed with only his snazzy attire, a revolver under his jacket, and a quick-thinking mind, Valentin sets out in pursuit of the giant criminal whose doodles of daring have made him a towering figure in the world of crime. Balancing official decorum with worldly charm, Valentin navigates through twisty paths, searching for Flambeau among ordinary folks and listening for whispers that prick sharper than daggers.\\n\\nNow, swirl with those butterflies of anticipation as Flambeau, the charming trickster, operates under an alias, all while plotting to snatch Father Brownâ€™s valuable sapphire cross. In a thrilling sequence of nearly inconspicuous encountersâ€”a mistaken sugar basin, cloaked clergymen, and a splodge of soup flung upon a wallâ€”Valentin finds himself on a dizzying set of errands that only serve to tighten the playful web Flambeau spins around his hunt. With shenanigans that turn a policeman's reserve into gasping camaraderie, every twist zooms you in closer to the ultimate showdown.\\n\\nAs the engagement unfolds, the heart of the chase embodies not just comic relief and clever disguises, but an insight into morality lurking beneath snickers. Father Brownâ€”incognito beneath that sheepish clerical garbâ€”matches Flambeau every step with intellectual finesse and a keen eye for the sin beneath mischievous fronts. In the climax, where wit bows to wisdom, we discover that while troubles may breed outrageous laughter and laughter can spark thoughtless folly, the mind more attuned to morals tips triumphantly in conclusion.\\n\\nOh, what a treasure this story is! ðŸŽ‰ With flamboyant characters woven together by dainty fables and insights into good and evil, it leaves us chuckling and reflectingâ€”sometimes itâ€™s not about capturing the essence of crime, but revealing the heart and humor nestled within them. I invite you to ride this intriguing wave of whimsy; youâ€™d surely agree that every story, no matter how lighthearted, has a profound truth hidden beneath its kisses of cleverness!\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.output_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcae503b",
   "metadata": {},
   "source": [
    "# Answer Relevancy\n",
    "\n",
    "The answer relevancy metric evaluates how relevant the actual output of the LLM app is compared to the provided input. This metric is self-explaining in the sense that the output includes a reason for the metric score.\n",
    "\n",
    "The metric is calculated as:\n",
    "\n",
    "$$\n",
    "AnswerRelevancy=\\frac{NumberRelevantStatements}{TotalStatements}\n",
    "$$\n",
    "\n",
    "Reference: [Answer Relevancy](https://deepeval.com/docs/metrics-answer-relevancy). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7cb6209",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval import evaluate\n",
    "from deepeval.metrics import AnswerRelevancyMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "metric = AnswerRelevancyMetric(\n",
    "    threshold=0.7,\n",
    "    model=\"gpt-4o-mini\",\n",
    "    include_reason=True\n",
    ")\n",
    "\n",
    "test_case = LLMTestCase(\n",
    "    input=PROMPT.format(story=blue_cross_text),\n",
    "    actual_output=response.output_text\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd7de7ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19bd0472f5a74240959a6aa8dd820e07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0.9230769230769231"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metric.measure(test_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c73c9df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9230769230769231 The score is 0.92 because while the summary effectively captures the key characters and plot points of the story, it incorrectly states that Flambeau operates under an alias, which is not true as he is known by his real name. This minor inaccuracy prevents the score from being higher, but the overall quality of the summary and the inclusion of essential details justify the current score.\n"
     ]
    }
   ],
   "source": [
    "print(metric.score,metric.reason)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa92303",
   "metadata": {},
   "source": [
    "# Other Metrics\n",
    "\n",
    "Other useful metric functions include:\n",
    "\n",
    "+ [Faithfulness](https://deepeval.com/docs/metrics-faithfulness): evaluates whether the `actual_output` factually aligns with the contents of  `retrieval_context`. \n",
    "+ [Contextual Precision](https://deepeval.com/docs/metrics-contextual-precision): evaluates whether nodes in your `retrieval_context` that are relevant to the given input are ranked higher than irrelevant ones. \n",
    "+ [Contextual Recall](https://deepeval.com/docs/metrics-contextual-recall): evaluates the extent of which the retrieval_context aligns with the expected_output. \n",
    "+ [Contextual Relevancy](https://deepeval.com/docs/metrics-contextual-relevancy): evaluates the overall relevance of the information presented in your retrieval_context for a given input. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e7b6ba",
   "metadata": {},
   "source": [
    "# G-Eval\n",
    "\n",
    "[G-Eval](https://deepeval.com/docs/metrics-llm-evals) is a framework that uses LLM-as-a-judge with chain-of-thoughts (CoT) to evaluate LLM outputs based on ANY custom criteria. The G-Eval metric is the most versatile type of metric deepeval offers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f34e63bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "instructions = \"You are an helpful assistant that specializes in works of fiction.\"\n",
    "PROMPT = \"\"\"\n",
    "    Based on the story below, answer the question provided.\n",
    "    <story>\n",
    "    {story}\n",
    "    </story>\n",
    "    <question>\n",
    "    Who is the main antagonist in the story and what motivates their actions?\n",
    "    </question>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2df04e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI()\n",
    "response = client.responses.create(\n",
    "    model=\"gpt-4o\",\n",
    "    instructions=instructions,\n",
    "    input=[\n",
    "        {\"role\": \"user\", \n",
    "         \"content\": PROMPT.format(story=blue_cross_text)}\n",
    "    ],\n",
    "    temperature=0.7\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7833729e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The main antagonist in the story is Flambeau, a notorious and ingenious criminal. His actions are motivated by his desire to commit high-profile thefts, such as stealing the valuable sapphire cross in this story. Flambeau is known for his creative and daring criminal exploits, often employing clever disguises and elaborate schemes to achieve his goals. His motivation is primarily driven by the thrill of the crime and the challenge it presents, as well as the potential for significant gain.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.output_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0eb8ad",
   "metadata": {},
   "source": [
    "## Evaluation Criteria\n",
    "\n",
    "The most straightforward way to establish a metric is by using a single criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9a310a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepeval.metrics import GEval\n",
    "from deepeval.test_case import LLMTestCaseParams\n",
    "\n",
    "correctness_metric = GEval(\n",
    "    name=\"Correctness\",\n",
    "    criteria=\"Determine whether the actual output is factually correct based on the context.\",\n",
    "    evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b0312796",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">âœ¨ You're running DeepEval's latest <span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">Correctness </span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">[</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\">GEval</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff; font-weight: bold\">]</span><span style=\"color: #6a00ff; text-decoration-color: #6a00ff\"> Metric</span>! <span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">(</span><span style=\"color: #374151; text-decoration-color: #374151\">using gpt-</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">4.1</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">strict</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">False</span><span style=\"color: #374151; text-decoration-color: #374151\">, </span><span style=\"color: #374151; text-decoration-color: #374151\">async_mode</span><span style=\"color: #374151; text-decoration-color: #374151\">=</span><span style=\"color: #374151; text-decoration-color: #374151; font-style: italic\">True</span><span style=\"color: #374151; text-decoration-color: #374151; font-weight: bold\">)</span><span style=\"color: #374151; text-decoration-color: #374151\">...</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "âœ¨ You're running DeepEval's latest \u001b[38;2;106;0;255mCorrectness \u001b[0m\u001b[1;38;2;106;0;255m[\u001b[0m\u001b[38;2;106;0;255mGEval\u001b[0m\u001b[1;38;2;106;0;255m]\u001b[0m\u001b[38;2;106;0;255m Metric\u001b[0m! \u001b[1;38;2;55;65;81m(\u001b[0m\u001b[38;2;55;65;81musing gpt-\u001b[0m\u001b[1;38;2;55;65;81m4.1\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81mstrict\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mFalse\u001b[0m\u001b[38;2;55;65;81m, \u001b[0m\u001b[38;2;55;65;81masync_mode\u001b[0m\u001b[38;2;55;65;81m=\u001b[0m\u001b[3;38;2;55;65;81mTrue\u001b[0m\u001b[1;38;2;55;65;81m)\u001b[0m\u001b[38;2;55;65;81m...\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03031142d71c4dc586edb7d0ee4d51d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "PermissionDeniedError",
     "evalue": "Error code: 403 - {'error': {'message': 'Project `proj_azcDlGrYmDy6eV8yO8hoT2pv` does not have access to model `gpt-4.1`', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPermissionDeniedError\u001b[39m                     Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m test_case = LLMTestCase(\n\u001b[32m      2\u001b[39m     \u001b[38;5;28minput\u001b[39m=PROMPT.format(story=blue_cross_text),\n\u001b[32m      3\u001b[39m     actual_output=response.output_text\n\u001b[32m      4\u001b[39m )\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_cases\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtest_case\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcorrectness_metric\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ramin\\DSI\\deploying-ai\\deploying-ai-env\\Lib\\site-packages\\deepeval\\evaluate\\evaluate.py:229\u001b[39m, in \u001b[36mevaluate\u001b[39m\u001b[34m(test_cases, metrics, metric_collection, hyperparameters, identifier, async_config, display_config, cache_config, error_config)\u001b[39m\n\u001b[32m    227\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m async_config.run_async:\n\u001b[32m    228\u001b[39m     loop = get_or_create_event_loop()\n\u001b[32m--> \u001b[39m\u001b[32m229\u001b[39m     test_results = \u001b[43mloop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_until_complete\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    230\u001b[39m \u001b[43m        \u001b[49m\u001b[43ma_execute_test_cases\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    231\u001b[39m \u001b[43m            \u001b[49m\u001b[43mtest_cases\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    233\u001b[39m \u001b[43m            \u001b[49m\u001b[43midentifier\u001b[49m\u001b[43m=\u001b[49m\u001b[43midentifier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    234\u001b[39m \u001b[43m            \u001b[49m\u001b[43mignore_errors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merror_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mignore_errors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    235\u001b[39m \u001b[43m            \u001b[49m\u001b[43mskip_on_missing_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43merror_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mskip_on_missing_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    236\u001b[39m \u001b[43m            \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m            \u001b[49m\u001b[43msave_to_disk\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m            \u001b[49m\u001b[43mverbose_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisplay_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mverbose_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[43m            \u001b[49m\u001b[43mshow_indicator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdisplay_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshow_indicator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    240\u001b[39m \u001b[43m            \u001b[49m\u001b[43mthrottle_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43masync_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mthrottle_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    241\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmax_concurrent\u001b[49m\u001b[43m=\u001b[49m\u001b[43masync_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmax_concurrent\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    242\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    243\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    244\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    245\u001b[39m     test_results = execute_test_cases(\n\u001b[32m    246\u001b[39m         test_cases,\n\u001b[32m    247\u001b[39m         metrics,\n\u001b[32m   (...)\u001b[39m\u001b[32m    254\u001b[39m         verbose_mode=display_config.verbose_mode,\n\u001b[32m    255\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ramin\\DSI\\deploying-ai\\deploying-ai-env\\Lib\\site-packages\\nest_asyncio.py:98\u001b[39m, in \u001b[36m_patch_loop.<locals>.run_until_complete\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m     95\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m f.done():\n\u001b[32m     96\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m     97\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mEvent loop stopped before Future completed.\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m98\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\futures.py:203\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    201\u001b[39m \u001b[38;5;28mself\u001b[39m.__log_traceback = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    202\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception.with_traceback(\u001b[38;5;28mself\u001b[39m._exception_tb)\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py:316\u001b[39m, in \u001b[36mTask.__step_run_and_handle_result\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    314\u001b[39m         result = coro.send(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    315\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m         result = \u001b[43mcoro\u001b[49m\u001b[43m.\u001b[49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    318\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._must_cancel:\n\u001b[32m    319\u001b[39m         \u001b[38;5;66;03m# Task is cancelled right before coro stops.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ramin\\DSI\\deploying-ai\\deploying-ai-env\\Lib\\site-packages\\deepeval\\evaluate\\execute.py:553\u001b[39m, in \u001b[36ma_execute_test_cases\u001b[39m\u001b[34m(test_cases, metrics, ignore_errors, skip_on_missing_params, use_cache, show_indicator, throttle_value, max_concurrent, save_to_disk, verbose_mode, identifier, test_run_manager, _use_bar_indicator, _is_assert_test)\u001b[39m\n\u001b[32m    550\u001b[39m                     tasks.append(asyncio.create_task(task))\n\u001b[32m    552\u001b[39m                 \u001b[38;5;28;01mawait\u001b[39;00m asyncio.sleep(throttle_value)\n\u001b[32m--> \u001b[39m\u001b[32m553\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(*tasks)\n\u001b[32m    554\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    555\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m test_case \u001b[38;5;129;01min\u001b[39;00m test_cases:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py:385\u001b[39m, in \u001b[36mTask.__wakeup\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m    383\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__wakeup\u001b[39m(\u001b[38;5;28mself\u001b[39m, future):\n\u001b[32m    384\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m385\u001b[39m         \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    386\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    387\u001b[39m         \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n\u001b[32m    388\u001b[39m         \u001b[38;5;28mself\u001b[39m.__step(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py:316\u001b[39m, in \u001b[36mTask.__step_run_and_handle_result\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    314\u001b[39m         result = coro.send(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    315\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m316\u001b[39m         result = \u001b[43mcoro\u001b[49m\u001b[43m.\u001b[49m\u001b[43mthrow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    318\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._must_cancel:\n\u001b[32m    319\u001b[39m         \u001b[38;5;66;03m# Task is cancelled right before coro stops.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ramin\\DSI\\deploying-ai\\deploying-ai-env\\Lib\\site-packages\\deepeval\\evaluate\\execute.py:436\u001b[39m, in \u001b[36ma_execute_test_cases.<locals>.execute_with_semaphore\u001b[39m\u001b[34m(func, *args, **kwargs)\u001b[39m\n\u001b[32m    434\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mexecute_with_semaphore\u001b[39m(func: Callable, *args, **kwargs):\n\u001b[32m    435\u001b[39m     \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mwith\u001b[39;00m semaphore:\n\u001b[32m--> \u001b[39m\u001b[32m436\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m func(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ramin\\DSI\\deploying-ai\\deploying-ai-env\\Lib\\site-packages\\deepeval\\evaluate\\execute.py:672\u001b[39m, in \u001b[36ma_execute_llm_test_cases\u001b[39m\u001b[34m(metrics, test_case, test_run_manager, test_results, count, test_run, ignore_errors, skip_on_missing_params, use_cache, show_indicator, _use_bar_indicator, _is_assert_test, progress, pbar_id)\u001b[39m\n\u001b[32m    670\u001b[39m new_cached_test_case: CachedTestCase = CachedTestCase()\n\u001b[32m    671\u001b[39m test_start_time = time.perf_counter()\n\u001b[32m--> \u001b[39m\u001b[32m672\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m measure_metrics_with_indicator(\n\u001b[32m    673\u001b[39m     metrics=metrics,\n\u001b[32m    674\u001b[39m     test_case=test_case,\n\u001b[32m    675\u001b[39m     cached_test_case=cached_test_case,\n\u001b[32m    676\u001b[39m     skip_on_missing_params=skip_on_missing_params,\n\u001b[32m    677\u001b[39m     ignore_errors=ignore_errors,\n\u001b[32m    678\u001b[39m     show_indicator=show_metrics_indicator,\n\u001b[32m    679\u001b[39m     pbar_eval_id=pbar_test_case_id,\n\u001b[32m    680\u001b[39m     progress=progress,\n\u001b[32m    681\u001b[39m )\n\u001b[32m    683\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m metric \u001b[38;5;129;01min\u001b[39;00m metrics:\n\u001b[32m    684\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m metric.skipped:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ramin\\DSI\\deploying-ai\\deploying-ai-env\\Lib\\site-packages\\deepeval\\metrics\\indicator.py:230\u001b[39m, in \u001b[36mmeasure_metrics_with_indicator\u001b[39m\u001b[34m(metrics, test_case, cached_test_case, ignore_errors, skip_on_missing_params, show_indicator, progress, pbar_eval_id, _in_component)\u001b[39m\n\u001b[32m    217\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    218\u001b[39m         tasks.append(\n\u001b[32m    219\u001b[39m             safe_a_measure(\n\u001b[32m    220\u001b[39m                 metric,\n\u001b[32m   (...)\u001b[39m\u001b[32m    227\u001b[39m             )\n\u001b[32m    228\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m230\u001b[39m \u001b[38;5;28;01mawait\u001b[39;00m asyncio.gather(*tasks)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py:385\u001b[39m, in \u001b[36mTask.__wakeup\u001b[39m\u001b[34m(self, future)\u001b[39m\n\u001b[32m    383\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__wakeup\u001b[39m(\u001b[38;5;28mself\u001b[39m, future):\n\u001b[32m    384\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m385\u001b[39m         \u001b[43mfuture\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    386\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[32m    387\u001b[39m         \u001b[38;5;66;03m# This may also be a cancellation.\u001b[39;00m\n\u001b[32m    388\u001b[39m         \u001b[38;5;28mself\u001b[39m.__step(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\asyncio\\tasks.py:314\u001b[39m, in \u001b[36mTask.__step_run_and_handle_result\u001b[39m\u001b[34m(***failed resolving arguments***)\u001b[39m\n\u001b[32m    310\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    311\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    312\u001b[39m         \u001b[38;5;66;03m# We use the `send` method directly, because coroutines\u001b[39;00m\n\u001b[32m    313\u001b[39m         \u001b[38;5;66;03m# don't have `__iter__` and `__next__` methods.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m314\u001b[39m         result = \u001b[43mcoro\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    315\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    316\u001b[39m         result = coro.throw(exc)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ramin\\DSI\\deploying-ai\\deploying-ai-env\\Lib\\site-packages\\deepeval\\metrics\\indicator.py:243\u001b[39m, in \u001b[36msafe_a_measure\u001b[39m\u001b[34m(metric, tc, ignore_errors, skip_on_missing_params, progress, pbar_eval_id, _in_component)\u001b[39m\n\u001b[32m    233\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msafe_a_measure\u001b[39m(\n\u001b[32m    234\u001b[39m     metric: Union[BaseMetric, BaseMultimodalMetric, BaseConversationalMetric],\n\u001b[32m    235\u001b[39m     tc: Union[LLMTestCase, MLLMTestCase, ConversationalTestCase],\n\u001b[32m   (...)\u001b[39m\u001b[32m    240\u001b[39m     _in_component: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    241\u001b[39m ):\n\u001b[32m    242\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m243\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m metric.a_measure(\n\u001b[32m    244\u001b[39m             tc, _show_indicator=\u001b[38;5;28;01mFalse\u001b[39;00m, _in_component=_in_component\n\u001b[32m    245\u001b[39m         )\n\u001b[32m    246\u001b[39m         update_pbar(progress, pbar_eval_id)\n\u001b[32m    247\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m MissingTestCaseParamsError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ramin\\DSI\\deploying-ai\\deploying-ai-env\\Lib\\site-packages\\deepeval\\metrics\\g_eval\\g_eval.py:135\u001b[39m, in \u001b[36mGEval.a_measure\u001b[39m\u001b[34m(self, test_case, _show_indicator, _in_component, _additional_context)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28mself\u001b[39m.evaluation_cost = \u001b[32m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.using_native_model \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    128\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m metric_progress_indicator(\n\u001b[32m    129\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    130\u001b[39m     async_mode=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    131\u001b[39m     _show_indicator=_show_indicator,\n\u001b[32m    132\u001b[39m     _in_component=_in_component,\n\u001b[32m    133\u001b[39m ):\n\u001b[32m    134\u001b[39m     \u001b[38;5;28mself\u001b[39m.evaluation_steps: List[\u001b[38;5;28mstr\u001b[39m] = (\n\u001b[32m--> \u001b[39m\u001b[32m135\u001b[39m         \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._a_generate_evaluation_steps()\n\u001b[32m    136\u001b[39m     )\n\u001b[32m    137\u001b[39m     g_score, reason = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._a_evaluate(\n\u001b[32m    138\u001b[39m         test_case, _additional_context=_additional_context\n\u001b[32m    139\u001b[39m     )\n\u001b[32m    140\u001b[39m     \u001b[38;5;28mself\u001b[39m.score = (\n\u001b[32m    141\u001b[39m         \u001b[38;5;28mfloat\u001b[39m(g_score) / \u001b[38;5;28mself\u001b[39m.score_range_span\n\u001b[32m    142\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.strict_mode\n\u001b[32m    143\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mint\u001b[39m(g_score)\n\u001b[32m    144\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ramin\\DSI\\deploying-ai\\deploying-ai-env\\Lib\\site-packages\\deepeval\\metrics\\g_eval\\g_eval.py:171\u001b[39m, in \u001b[36mGEval._a_generate_evaluation_steps\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    167\u001b[39m prompt = GEvalTemplate.generate_evaluation_steps(\n\u001b[32m    168\u001b[39m     criteria=\u001b[38;5;28mself\u001b[39m.criteria, parameters=g_eval_params_str\n\u001b[32m    169\u001b[39m )\n\u001b[32m    170\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.using_native_model:\n\u001b[32m--> \u001b[39m\u001b[32m171\u001b[39m     res, cost = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model.a_generate(prompt)\n\u001b[32m    172\u001b[39m     \u001b[38;5;28mself\u001b[39m.evaluation_cost += cost\n\u001b[32m    173\u001b[39m     data = trimAndLoadJson(res, \u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ramin\\DSI\\deploying-ai\\deploying-ai-env\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:189\u001b[39m, in \u001b[36mAsyncRetrying.wraps.<locals>.async_wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    187\u001b[39m copy = \u001b[38;5;28mself\u001b[39m.copy()\n\u001b[32m    188\u001b[39m async_wrapped.statistics = copy.statistics  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m copy(fn, *args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ramin\\DSI\\deploying-ai\\deploying-ai-env\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:111\u001b[39m, in \u001b[36mAsyncRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    109\u001b[39m retry_state = RetryCallState(retry_object=\u001b[38;5;28mself\u001b[39m, fn=fn, args=args, kwargs=kwargs)\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m111\u001b[39m     do = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter(retry_state=retry_state)\n\u001b[32m    112\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    113\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ramin\\DSI\\deploying-ai\\deploying-ai-env\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:153\u001b[39m, in \u001b[36mAsyncRetrying.iter\u001b[39m\u001b[34m(self, retry_state)\u001b[39m\n\u001b[32m    151\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m action \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.actions:\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m     result = \u001b[38;5;28;01mawait\u001b[39;00m action(retry_state)\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ramin\\DSI\\deploying-ai\\deploying-ai-env\\Lib\\site-packages\\tenacity\\_utils.py:99\u001b[39m, in \u001b[36mwrap_to_async_func.<locals>.inner\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     98\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34minner\u001b[39m(*args: typing.Any, **kwargs: typing.Any) -> typing.Any:\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ramin\\DSI\\deploying-ai\\deploying-ai-env\\Lib\\site-packages\\tenacity\\__init__.py:400\u001b[39m, in \u001b[36mBaseRetrying._post_retry_check_actions.<locals>.<lambda>\u001b[39m\u001b[34m(rs)\u001b[39m\n\u001b[32m    398\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_post_retry_check_actions\u001b[39m(\u001b[38;5;28mself\u001b[39m, retry_state: \u001b[33m\"\u001b[39m\u001b[33mRetryCallState\u001b[39m\u001b[33m\"\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    399\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.iter_state.is_explicit_retry \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.iter_state.retry_run_result):\n\u001b[32m--> \u001b[39m\u001b[32m400\u001b[39m         \u001b[38;5;28mself\u001b[39m._add_action_func(\u001b[38;5;28;01mlambda\u001b[39;00m rs: \u001b[43mrs\u001b[49m\u001b[43m.\u001b[49m\u001b[43moutcome\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    401\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    403\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.after \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:449\u001b[39m, in \u001b[36mFuture.result\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    447\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[32m    448\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state == FINISHED:\n\u001b[32m--> \u001b[39m\u001b[32m449\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[38;5;28mself\u001b[39m._condition.wait(timeout)\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._state \u001b[38;5;129;01min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\concurrent\\futures\\_base.py:401\u001b[39m, in \u001b[36mFuture.__get_result\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception:\n\u001b[32m    400\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m401\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._exception\n\u001b[32m    402\u001b[39m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    403\u001b[39m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[32m    404\u001b[39m         \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ramin\\DSI\\deploying-ai\\deploying-ai-env\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py:114\u001b[39m, in \u001b[36mAsyncRetrying.__call__\u001b[39m\u001b[34m(self, fn, *args, **kwargs)\u001b[39m\n\u001b[32m    112\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(do, DoAttempt):\n\u001b[32m    113\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m         result = \u001b[38;5;28;01mawait\u001b[39;00m fn(*args, **kwargs)\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m:  \u001b[38;5;66;03m# noqa: B902\u001b[39;00m\n\u001b[32m    116\u001b[39m         retry_state.set_exception(sys.exc_info())  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ramin\\DSI\\deploying-ai\\deploying-ai-env\\Lib\\site-packages\\deepeval\\models\\llms\\openai_model.py:352\u001b[39m, in \u001b[36mGPTModel.a_generate\u001b[39m\u001b[34m(self, prompt, schema)\u001b[39m\n\u001b[32m    346\u001b[39m         cost = \u001b[38;5;28mself\u001b[39m.calculate_cost(\n\u001b[32m    347\u001b[39m             completion.usage.prompt_tokens,\n\u001b[32m    348\u001b[39m             completion.usage.completion_tokens,\n\u001b[32m    349\u001b[39m         )\n\u001b[32m    350\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m schema.model_validate(json_output), cost\n\u001b[32m--> \u001b[39m\u001b[32m352\u001b[39m completion = \u001b[38;5;28;01mawait\u001b[39;00m client.chat.completions.create(\n\u001b[32m    353\u001b[39m     model=\u001b[38;5;28mself\u001b[39m.model_name,\n\u001b[32m    354\u001b[39m     messages=[{\u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcontent\u001b[39m\u001b[33m\"\u001b[39m: prompt}],\n\u001b[32m    355\u001b[39m     temperature=\u001b[38;5;28mself\u001b[39m.temperature,\n\u001b[32m    356\u001b[39m )\n\u001b[32m    357\u001b[39m output = completion.choices[\u001b[32m0\u001b[39m].message.content\n\u001b[32m    358\u001b[39m cost = \u001b[38;5;28mself\u001b[39m.calculate_cost(\n\u001b[32m    359\u001b[39m     completion.usage.prompt_tokens, completion.usage.completion_tokens\n\u001b[32m    360\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ramin\\DSI\\deploying-ai\\deploying-ai-env\\Lib\\site-packages\\openai\\resources\\chat\\completions\\completions.py:2585\u001b[39m, in \u001b[36mAsyncCompletions.create\u001b[39m\u001b[34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, prompt_cache_key, reasoning_effort, response_format, safety_identifier, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, verbosity, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[39m\n\u001b[32m   2539\u001b[39m \u001b[38;5;129m@required_args\u001b[39m([\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m], [\u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m])\n\u001b[32m   2540\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcreate\u001b[39m(\n\u001b[32m   2541\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2582\u001b[39m     timeout: \u001b[38;5;28mfloat\u001b[39m | httpx.Timeout | \u001b[38;5;28;01mNone\u001b[39;00m | NotGiven = not_given,\n\u001b[32m   2583\u001b[39m ) -> ChatCompletion | AsyncStream[ChatCompletionChunk]:\n\u001b[32m   2584\u001b[39m     validate_response_format(response_format)\n\u001b[32m-> \u001b[39m\u001b[32m2585\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m._post(\n\u001b[32m   2586\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m/chat/completions\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2587\u001b[39m         body=\u001b[38;5;28;01mawait\u001b[39;00m async_maybe_transform(\n\u001b[32m   2588\u001b[39m             {\n\u001b[32m   2589\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m\"\u001b[39m: messages,\n\u001b[32m   2590\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodel\u001b[39m\u001b[33m\"\u001b[39m: model,\n\u001b[32m   2591\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33maudio\u001b[39m\u001b[33m\"\u001b[39m: audio,\n\u001b[32m   2592\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfrequency_penalty\u001b[39m\u001b[33m\"\u001b[39m: frequency_penalty,\n\u001b[32m   2593\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfunction_call\u001b[39m\u001b[33m\"\u001b[39m: function_call,\n\u001b[32m   2594\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mfunctions\u001b[39m\u001b[33m\"\u001b[39m: functions,\n\u001b[32m   2595\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mlogit_bias\u001b[39m\u001b[33m\"\u001b[39m: logit_bias,\n\u001b[32m   2596\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mlogprobs\u001b[39m\u001b[33m\"\u001b[39m: logprobs,\n\u001b[32m   2597\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_completion_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_completion_tokens,\n\u001b[32m   2598\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmax_tokens\u001b[39m\u001b[33m\"\u001b[39m: max_tokens,\n\u001b[32m   2599\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m: metadata,\n\u001b[32m   2600\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mmodalities\u001b[39m\u001b[33m\"\u001b[39m: modalities,\n\u001b[32m   2601\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mn\u001b[39m\u001b[33m\"\u001b[39m: n,\n\u001b[32m   2602\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mparallel_tool_calls\u001b[39m\u001b[33m\"\u001b[39m: parallel_tool_calls,\n\u001b[32m   2603\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprediction\u001b[39m\u001b[33m\"\u001b[39m: prediction,\n\u001b[32m   2604\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mpresence_penalty\u001b[39m\u001b[33m\"\u001b[39m: presence_penalty,\n\u001b[32m   2605\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mprompt_cache_key\u001b[39m\u001b[33m\"\u001b[39m: prompt_cache_key,\n\u001b[32m   2606\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mreasoning_effort\u001b[39m\u001b[33m\"\u001b[39m: reasoning_effort,\n\u001b[32m   2607\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mresponse_format\u001b[39m\u001b[33m\"\u001b[39m: response_format,\n\u001b[32m   2608\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33msafety_identifier\u001b[39m\u001b[33m\"\u001b[39m: safety_identifier,\n\u001b[32m   2609\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mseed\u001b[39m\u001b[33m\"\u001b[39m: seed,\n\u001b[32m   2610\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mservice_tier\u001b[39m\u001b[33m\"\u001b[39m: service_tier,\n\u001b[32m   2611\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstop\u001b[39m\u001b[33m\"\u001b[39m: stop,\n\u001b[32m   2612\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstore\u001b[39m\u001b[33m\"\u001b[39m: store,\n\u001b[32m   2613\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream\u001b[39m\u001b[33m\"\u001b[39m: stream,\n\u001b[32m   2614\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mstream_options\u001b[39m\u001b[33m\"\u001b[39m: stream_options,\n\u001b[32m   2615\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtemperature\u001b[39m\u001b[33m\"\u001b[39m: temperature,\n\u001b[32m   2616\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtool_choice\u001b[39m\u001b[33m\"\u001b[39m: tool_choice,\n\u001b[32m   2617\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtools\u001b[39m\u001b[33m\"\u001b[39m: tools,\n\u001b[32m   2618\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_logprobs\u001b[39m\u001b[33m\"\u001b[39m: top_logprobs,\n\u001b[32m   2619\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mtop_p\u001b[39m\u001b[33m\"\u001b[39m: top_p,\n\u001b[32m   2620\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m: user,\n\u001b[32m   2621\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mverbosity\u001b[39m\u001b[33m\"\u001b[39m: verbosity,\n\u001b[32m   2622\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mweb_search_options\u001b[39m\u001b[33m\"\u001b[39m: web_search_options,\n\u001b[32m   2623\u001b[39m             },\n\u001b[32m   2624\u001b[39m             completion_create_params.CompletionCreateParamsStreaming\n\u001b[32m   2625\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m stream\n\u001b[32m   2626\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m completion_create_params.CompletionCreateParamsNonStreaming,\n\u001b[32m   2627\u001b[39m         ),\n\u001b[32m   2628\u001b[39m         options=make_request_options(\n\u001b[32m   2629\u001b[39m             extra_headers=extra_headers, extra_query=extra_query, extra_body=extra_body, timeout=timeout\n\u001b[32m   2630\u001b[39m         ),\n\u001b[32m   2631\u001b[39m         cast_to=ChatCompletion,\n\u001b[32m   2632\u001b[39m         stream=stream \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m   2633\u001b[39m         stream_cls=AsyncStream[ChatCompletionChunk],\n\u001b[32m   2634\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ramin\\DSI\\deploying-ai\\deploying-ai-env\\Lib\\site-packages\\openai\\_base_client.py:1794\u001b[39m, in \u001b[36mAsyncAPIClient.post\u001b[39m\u001b[34m(self, path, cast_to, body, files, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1780\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpost\u001b[39m(\n\u001b[32m   1781\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   1782\u001b[39m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1789\u001b[39m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_AsyncStreamT] | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1790\u001b[39m ) -> ResponseT | _AsyncStreamT:\n\u001b[32m   1791\u001b[39m     opts = FinalRequestOptions.construct(\n\u001b[32m   1792\u001b[39m         method=\u001b[33m\"\u001b[39m\u001b[33mpost\u001b[39m\u001b[33m\"\u001b[39m, url=path, json_data=body, files=\u001b[38;5;28;01mawait\u001b[39;00m async_to_httpx_files(files), **options\n\u001b[32m   1793\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1794\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.request(cast_to, opts, stream=stream, stream_cls=stream_cls)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ramin\\DSI\\deploying-ai\\deploying-ai-env\\Lib\\site-packages\\openai\\_base_client.py:1594\u001b[39m, in \u001b[36mAsyncAPIClient.request\u001b[39m\u001b[34m(self, cast_to, options, stream, stream_cls)\u001b[39m\n\u001b[32m   1591\u001b[39m             \u001b[38;5;28;01mawait\u001b[39;00m err.response.aread()\n\u001b[32m   1593\u001b[39m         log.debug(\u001b[33m\"\u001b[39m\u001b[33mRe-raising status error\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1594\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_status_error_from_response(err.response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1596\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   1598\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[33m\"\u001b[39m\u001b[33mcould not resolve response (should never happen)\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mPermissionDeniedError\u001b[39m: Error code: 403 - {'error': {'message': 'Project `proj_azcDlGrYmDy6eV8yO8hoT2pv` does not have access to model `gpt-4.1`', 'type': 'invalid_request_error', 'param': None, 'code': 'model_not_found'}}"
     ]
    }
   ],
   "source": [
    "test_case = LLMTestCase(\n",
    "    input=PROMPT.format(story=blue_cross_text),\n",
    "    actual_output=response.output_text\n",
    ")\n",
    "evaluate(test_cases=[test_case], metrics=[correctness_metric])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9542714",
   "metadata": {},
   "source": [
    "## Evaluation Steps \n",
    "\n",
    "G-Eval is flexible in many ways: notice that we can establish an evaluation criteria or a set of evaluation steps, that can help in guiding the model to follow specific steps to perform the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8647c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "...\n",
    "\n",
    "correctness_metric = GEval(\n",
    "    name=\"Correctness\",\n",
    "    evaluation_steps=[\n",
    "        \"Check whether the facts in 'actual output' contradicts any facts in 'input'\",\n",
    "        \"You should also heavily penalize omission of detail\",\n",
    "        \"Vague language, or contradicting OPINIONS, are not OK\"\n",
    "    ],\n",
    "    evaluation_params=[LLMTestCaseParams.INPUT, LLMTestCaseParams.ACTUAL_OUTPUT],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e44fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_case = LLMTestCase(\n",
    "    input=PROMPT.format(story=blue_cross_text),\n",
    "    actual_output=response.output_text\n",
    ")\n",
    "evaluate(test_cases=[test_case], metrics=[correctness_metric])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deploying-ai-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
